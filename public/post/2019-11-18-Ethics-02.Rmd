---
title: "Reproducible Data Ethics Study"
author: "Stunty Godwit"
date: 2019-12-18
categories: ["Section 02", "Ethics Study"]
tags: ["Section 02", "Ethics Study"]
thumbnailImage: https://images.idgesg.net/images/article/2018/10/analytics_binary_code_network_digital-transformation-100777428-large.jpg
thumbnailImagePosition: left
---

Contrary to common knowledge, data from any type of study is very hard to replicate.  In addition to this, new studies form hypotheses based on previous study results.  Advancements in any field are built off robust and reproducible and replicable data and it has been discovered that only between 75% and 90% of all data is reproducible. The ethical issues are whether or not data and results should be published if they are not easily replicated. Continue to read this article to discover some factors that attribute to why data is so hard to reproduce and more on the ethics of this issue.



[Link to Similar Article](https://ed.ted.com/lessons/is-there-a-reproducibility-crisis-in-science-matt-anticole)

<!--more-->

When looking at published data, it is natural to assume that it was found to be reproducible. However, this is not necessarily the case. A large proportion of data (estimated to be roughly 75% to 90%) is actually unable to be replicated, both by third parties and the original people that conducted the study. There is an ethical debate among whether this should be corrected or is preferable to the alternative. An issue with publishing data that cannot be reproduced is that the data may not be the same each time something similar is tried. Any connections that the data hints at may be exceptions instead of rules, leading to faulty conclusions and actions. That being said, conducting the same study again to simply check if it works multiple times would cost a large amount of money for ultimately no gain (if a study is found to be unreproducible, then people still have no constructive conclusions about what actions to take in solving problems). This also connects to the desire for the media and the public to find some surprising correlation between variables. This way the media makes money and the public feels as though analysts are learning more about the world and helping them out. 

One example of unreproducible results being published was the cold fusion hoax in 1989. Electrochemists Stanley Pons of the University of Utah and his mentor, Martin Fleischmann of Britain's University of Southampton announced that they had discovered a simple way to recreate a nuclear reaction at room temperature using basic electrochemical techniques when it is typically only observed at extremely high temperatures. Right after this publication came out, several other scientists published similar findings that backed up the claim. There was one flaw, however. All the scientists who found this method to be successful were not experts in the field of quantitative isotope analysis, the subject of this experiment. Eventually, the theory was disproved completely, and even the original scientists were unable to recreate their experiment. In this case, people wanted to believe they had discovered a special, super-efficient energy device. This led to confirmation of findings spreading, and false reports were damaging to the ethics of science.

To insert images:

![](https://cdn.technologynetworks.com/tn/images/thumbs/jpeg/640_360/repeatability-vs-reproducibility-317157.jpg)