---
title: "Bias in AI"
author: "peewee_ocelot"
date: 2019-11-11
categories: ["Section 02", "Article Summary"]
tags: ["Section 02", "Bias in AI"]
thumbnailImage: https://miro.medium.com/max/5046/1*ZPSF6ifu1rIj-NwWlpH3kw.jpeg
thumbnailImagePosition: left
--- 
We put the artificial in artificial intelligence when it comes to statistics. We program robots to give us what we want, not 
representing our entire population. Let's use Aritificial Intelligence ethically and not incorporate
bias in our alogorithms. Who knows, maybe in the future robots will keep an eye on us, detecting our flaws in bias.

![](https://miro.medium.com/max/5046/1*ZPSF6ifu1rIj-NwWlpH3kw.jpeg)




AI algorithms, also known as deep learning, find patterns in the data we collect. Technologies like this are flawed with biases which we tend to ignore. The biases can be implemented at any time, ranging from when the data is collected to other stages in the deep learning process. 

The first AI bias is known as framing the problem. This happens when computer scientists are deciding what they want the deep learning model to achieve. Once they have decided this, they tweak the definition of what they are finding in order to satisfy their goal. For example, say the model is trying to find the nutritional facts of an apple for a company. If you only include the healthy ingredients such as protein, while ignoring the sugar and other unhealthy ingredients in the food, you are redefining the word nutritional facts as healthy facts about the apple. As a result, the apple would sell for the company because it’s healthy, making the model for business purposes rather than for a fair, overall assessment of the apple’s ingredients.

The way we collect data may also increase bias in the deep learning process. The first includes the data not representing the total population. It happens when a group of people’s opinions or votes are not taken into account. Furthermore, the bias could reflects existing prejudices. This was seen by Amazon when they realized their recruiting tool dismissed female candidates because it was trained on historical hiring decisions, which favored men over women. It was told in an article by Jeffery Danstin where he explained how Amazon’s computer models were trained to vet applicants by observing patterns in resumes submitted to the company over a 10-year period. Most came from men, a reflection of male dominance across the tech industry. 

The last type of bias happens when you prepare the data and choose what you want the algorithm to consider. An example could be seen in Amazon’s recruiting tool where they would consider the applicant's gender, education level, and years of experience more heavily. This is what people call the “art” of deep learning: choosing which attributes to consider or ignore which significantly influences your model’s prediction accuracy.

The bias is very difficult to fix. The first main reason is due to the unknown use of bias. Bias isn’t always obvious during a model’s construction because you may not realize the future impacts of your data and choices until much later. This was seen with Amazon’s recruiting technology when they didn’t know it was biased and it’s technology was geared more toward hiring men over women.

Another issue is that it is difficult to design a model which is applied to groups of people in different contexts. For example, it’s hard to design a fair computer model in California which would be applicable in Georgia due to the difference in the geographical areas. For instance, if you are designing a model for the greatest economical problems in the country, the problems would be different for California as compared to Georgia. Taking into account all these different people is difficult to do.

The last issue includes what the exact definition of fairness is. Fairness in computer science has to be defined in mathematical terms, like balancing the false positive and false negative rates of a prediction system. There are many different mathematical definitions of fairness that are also mutually exclusive. For example, should the same proportion of black and white individuals should get high risk assessment scores? Or that the same level of risk should result in the same score regardless of race? It’s impossible to fulfill both definitions at the same time. As you can see, the ambiguity of the word “fair” is hard to comprehend since we each may have a different definition to it.

Although the increase of bias is concerning, AI researchers are trying to fix these problems. They are trying to make algorithms to detect hidden biases. However, the definition of fairness and fixing discrimination in algorithmic systems is a long term issue and is still a work in progress in our AI society.

https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G
(Amazon’s AI Recruiting technology’s problem)
